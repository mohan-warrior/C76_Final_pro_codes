{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9428871,"sourceType":"datasetVersion","datasetId":5728037}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:15:52.987077Z","iopub.execute_input":"2025-04-04T09:15:52.987368Z","iopub.status.idle":"2025-04-04T09:16:09.816529Z","shell.execute_reply.started":"2025-04-04T09:15:52.987339Z","shell.execute_reply":"2025-04-04T09:16:09.815826Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def cost_function(params):\n    print(\"Evaluating parameters:\", params)\n    learning_rate = params[0]\n    dense_units = int(params[1])  # Cast to integer for Dense layer\n    # batch_size = int(params[1])   # Cast to integer for batch size\n\n    # Data generators\n    train_datagen = ImageDataGenerator(rescale=1./255)\n    val_datagen = ImageDataGenerator(rescale=1./255)\n\n    train_data = train_datagen.flow_from_directory(\n        '/kaggle/input/data-mal/data/train',\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='binary',\n        shuffle=True\n    )\n\n    val_data = val_datagen.flow_from_directory(\n        '/kaggle/input/data-mal/data/val',\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='binary'\n    )\n\n    # Model definition\n    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    model = models.Sequential([\n        base_model,\n        layers.Flatten(),\n        layers.Dense(dense_units, activation='relu'),\n        layers.Dense(1, activation='sigmoid')\n    ])\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n    model.compile(optimizer=optimizer,\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n    history = model.fit(\n        train_data,\n        epochs=50,\n        validation_data=val_data,\n        verbose=0\n    )\n\n    val_accuracy = max(history.history['val_accuracy'])\n    return val_accuracy  # Return positive accuracy for maximization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:16:09.817607Z","iopub.execute_input":"2025-04-04T09:16:09.818175Z","iopub.status.idle":"2025-04-04T09:16:09.824654Z","shell.execute_reply.started":"2025-04-04T09:16:09.818135Z","shell.execute_reply":"2025-04-04T09:16:09.823901Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Initial population\nmeow = np.array([\n    [0.001, 128],\n    [0.01, 64],\n    [0.0001, 256],\n    [0.005, 128],\n    [0.02, 96]\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:16:09.825931Z","iopub.execute_input":"2025-04-04T09:16:09.826244Z","iopub.status.idle":"2025-04-04T09:16:09.848383Z","shell.execute_reply.started":"2025-04-04T09:16:09.826216Z","shell.execute_reply":"2025-04-04T09:16:09.847733Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def jaya_optimization():\n    dim = 2  # Number of parameters\n    lb = [0.0001, 32]  # Lower bounds [learning rate, dense units]\n    ub = [0.1, 512]    # Upper bounds\n    population = meow.copy().astype(float)\n    \n    # Evaluate initial population\n    f = np.array([cost_function(ind) for ind in population])\n    \n    print(\"\\nInitial Fitness Values (f):\")\n    print(f)\n\n    for gen in range(4):\n        best_idx = np.argmax(f)\n        worst_idx = np.argmin(f)\n        best = population[best_idx]\n        worst = population[worst_idx]\n\n        new_population = np.zeros_like(population)\n        for i in range(len(population)):\n            # Generate r1 and r2 ONCE per individual (shared across parameters)\n            r1, r2 = np.random.rand(), np.random.rand()\n            # r1 = np.random.rand()\n            print(f\"\\nGeneration {gen+1}, Individual {i+1}: r1={r1:.4f}, r2={r2:.4f}\")\n            # print(f\"\\nGeneration {gen+1}, Individual {i+1}: r1={r1:.4f}, r2={1-r1:.4f}\")\n\n            for j in range(dim):\n                # Use the same r1 and r2 for both parameters\n                new_val = population[i, j] + r1*(best[j] - abs(population[i, j])) - r2*(worst[j] - abs(population[i, j]))\n                new_population[i, j] = np.clip(new_val, lb[j], ub[j])\n            \n            # Ensure dense units are integers\n            new_population[i, 1] = int(round(new_population[i, 1]))\n\n        # Print new population after clipping\n        print(f\"\\nGeneration {gen+1} - New Population (after clipping):\")\n        print(new_population)\n\n        # Evaluate new solutions\n        new_f = np.array([cost_function(ind) for ind in new_population])\n\n        # Print new fitness values\n        print(f\"\\nGeneration {gen+1} - New Fitness Values (new_f):\")\n        print(new_f)\n\n        # Update population if new solution is better\n        for i in range(len(population)):\n            if new_f[i] > f[i]:\n                population[i] = new_population[i]\n                f[i] = new_f[i]\n\n        best_acc = f[np.argmax(f)]\n        print(f\"\\nGeneration {gen+1}, Best Accuracy: {best_acc:.4f}\")\n\n    best_idx = np.argmax(f)\n    return population[best_idx], f[best_idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:16:09.849347Z","iopub.execute_input":"2025-04-04T09:16:09.849643Z","iopub.status.idle":"2025-04-04T09:16:09.866585Z","shell.execute_reply.started":"2025-04-04T09:16:09.849608Z","shell.execute_reply":"2025-04-04T09:16:09.865578Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Execute optimization\nbest_params, best_accuracy = jaya_optimization()\nprint(f\"Optimal Parameters: Learning Rate = {best_params[0]:.6f}, Dense Units = {int(best_params[1])}\")\nprint(f\"Highest Validation Accuracy: {best_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T09:16:09.867481Z","iopub.execute_input":"2025-04-04T09:16:09.867753Z","iopub.status.idle":"2025-04-04T12:46:22.985182Z","shell.execute_reply.started":"2025-04-04T09:16:09.867734Z","shell.execute_reply":"2025-04-04T12:46:22.984229Z"}},"outputs":[{"name":"stdout","text":"Evaluating parameters: [1.00e-03 1.28e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Evaluating parameters: [1.0e-02 6.4e+01]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.00e-04 2.56e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [5.00e-03 1.28e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [2.0e-02 9.6e+01]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\n\nInitial Fitness Values (f):\n[0.78608924 0.78608924 0.95013124 0.93963253 0.78608924]\n\nGeneration 1, Individual 1: r1=0.1151, r2=0.3221\n\nGeneration 1, Individual 2: r1=0.2981, r2=0.8819\n\nGeneration 1, Individual 3: r1=0.2509, r2=0.5951\n\nGeneration 1, Individual 4: r1=0.6366, r2=0.8812\n\nGeneration 1, Individual 5: r1=0.9961, r2=0.3686\n\nGeneration 1 - New Population (after clipping):\n[[8.96454479e-04 1.43000000e+02]\n [1.49854569e-02 6.50000000e+01]\n [1.00000000e-04 3.32000000e+02]\n [5.40562567e-03 2.09000000e+02]\n [7.18107527e-03 2.44000000e+02]]\nEvaluating parameters: [8.96454479e-04 1.43000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.49854569e-02 6.50000000e+01]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.00e-04 3.32e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [5.40562567e-03 2.09000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [7.18107527e-03 2.44000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\n\nGeneration 1 - New Fitness Values (new_f):\n[0.94619423 0.78608924 0.93569553 0.78608924 0.89501309]\n\nGeneration 1, Best Accuracy: 0.9501\n\nGeneration 2, Individual 1: r1=0.2047, r2=0.8631\n\nGeneration 2, Individual 2: r1=0.7141, r2=0.4423\n\nGeneration 2, Individual 3: r1=0.3702, r2=0.5569\n\nGeneration 2, Individual 4: r1=0.2207, r2=0.0439\n\nGeneration 2, Individual 5: r1=0.0752, r2=0.0608\n\nGeneration 2 - New Population (after clipping):\n[[1.00000000e-04 2.34000000e+02]\n [2.93037455e-03 2.01000000e+02]\n [1.00000000e-04 3.63000000e+02]\n [3.69933260e-03 1.59000000e+02]\n [6.47699369e-03 2.56000000e+02]]\nEvaluating parameters: [1.00e-04 2.34e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [2.93037455e-03 2.01000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.00e-04 3.63e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [3.6993326e-03 1.5900000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [6.47699369e-03 2.56000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\n\nGeneration 2 - New Fitness Values (new_f):\n[0.94356954 0.94225723 0.93175852 0.95013124 0.89238846]\n\nGeneration 2, Best Accuracy: 0.9501\n\nGeneration 3, Individual 1: r1=0.6355, r2=0.8083\n\nGeneration 3, Individual 2: r1=0.8644, r2=0.3816\n\nGeneration 3, Individual 3: r1=0.9968, r2=0.0315\n\nGeneration 3, Individual 4: r1=0.6200, r2=0.9648\n\nGeneration 3, Individual 5: r1=0.0355, r2=0.7476\n\nGeneration 3 - New Population (after clipping):\n[[1.00000000e-04 1.33000000e+02]\n [1.00000000e-04 2.32000000e+02]\n [1.00000000e-04 2.56000000e+02]\n [1.00000000e-04 1.37000000e+02]\n [6.92941973e-03 2.44000000e+02]]\nEvaluating parameters: [1.00e-04 1.33e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.00e-04 2.32e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.00e-04 2.56e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.00e-04 1.37e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [6.92941973e-03 2.44000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\n\nGeneration 3 - New Fitness Values (new_f):\n[0.94094491 0.94225723 0.94750655 0.94356954 0.78608924]\n\nGeneration 3, Best Accuracy: 0.9501\n\nGeneration 4, Individual 1: r1=0.8533, r2=0.8404\n\nGeneration 4, Individual 2: r1=0.4595, r2=0.2194\n\nGeneration 4, Individual 3: r1=0.0889, r2=0.0197\n\nGeneration 4, Individual 4: r1=0.0483, r2=0.1720\n\nGeneration 4, Individual 5: r1=0.8679, r2=0.7428\n\nGeneration 4 - New Population (after clipping):\n[[1.00000000e-04 1.55000000e+02]\n [6.97044493e-04 2.17000000e+02]\n [1.00000000e-04 2.56000000e+02]\n [2.92697417e-03 1.49000000e+02]\n [1.03546412e-03 2.54000000e+02]]\nEvaluating parameters: [1.00e-04 1.55e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [6.97044493e-04 2.17000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.00e-04 2.56e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [2.92697417e-03 1.49000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\nEvaluating parameters: [1.03546412e-03 2.54000000e+02]\nFound 2288 images belonging to 2 classes.\nFound 762 images belonging to 2 classes.\n\nGeneration 4 - New Fitness Values (new_f):\n[0.94225723 0.94619423 0.9225722  0.78608924 0.94750655]\n\nGeneration 4, Best Accuracy: 0.9501\nOptimal Parameters: Learning Rate = 0.000100, Dense Units = 256\nHighest Validation Accuracy: 0.9501\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}